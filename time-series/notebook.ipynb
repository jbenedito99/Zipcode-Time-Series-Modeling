{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flatiron Phase 4 Project\n",
    "\n",
    "* <b>Name:</b> James Benedito\n",
    "* <b>Pace:</b> Part-Time\n",
    "* <b>Instructor:</b> Morgan Jones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract\n",
    "\n",
    "In this Jupyter notebook, I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Business Problem\n",
    "\n",
    "When it comes to business in general, it is important to make informed decisions on where to invest funds because a high ROI is imperative for a company to thrive. Data is a powerful tool that can be leveraged to determine where huge profits can potentially be gained in the future. \n",
    "\n",
    "This Jupyter notebook will examine time series data from Zillow to highlight the <b>Top 5 zipcodes in Clark County, Nevada</b> to invest in. Clark County is the largest district in Nevada state, encompassing all of Las Vegas, and is the fourth-largest district in the United States. My theoretical stakeholder is a real estate company located in Las Vegas, Nevada, who are seeking out the best locations for their clients. \n",
    "\n",
    "My suggestions to the real estate company will be informed by an <b>ARIMA model</b> with optimized parameters. This chosen ARIMA model will be used for <b>forecasting</b>, using previous ROI information to project potential ROI for various zipcodes in the Clark County area. From this <b>projected ROI</b> metric, I will construct my recommendations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset \n",
    "\n",
    "The data used for this project comes from Zillow and is stored in a file called <b>zillow_data.csv</b>. The dataset shows mean housing prices over time, with each row representing a specific zipcode in a particular US city. As mentioned previously, I will be focusing on merely a subset of this data. The dataframe will be filtered so that it only includes houses located in Nevada. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start, I will explore the Zillow dataset to see how it is set up. From <b>.head()</b>, we see that the dataset has <b>272 columns</b>. Most of these columns represent dates in time and house the median price of houses for a particular zipcode. The other columns include <b>RegionID</b>, <b>RegionName</b>, <b>City</b>, <b>State</b>, <b>Metro</b>, <b>CountyName</b>, and <b>SizeRank</b>. We see that the data is quite expansive, encompassing information from many different cities and counties across the United States. As mentioned earlier, I want to filter the data so that it focuses merely on zipcodes in <b>Nevada</b> state because that is where the theoretical real estate company is based. From the main dataframe <b>zillow_df</b>, I will create a subset called <b>nv_df</b> that only has data on Nevada zipcodes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'int' from 'numpy' (C:\\Users\\micha\\anaconda3\\envs\\learn-env\\lib\\site-packages\\numpy\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-4bee624262b5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mstatsmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapi\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'matplotlib'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'inline'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\statsmodels\\api.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 78\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdistributions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miolib\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mregression\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrobust\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtools\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     79\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m from statsmodels._version import (\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\statsmodels\\distributions\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mstatsmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtools\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_testing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPytestTester\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m from .empirical_distribution import (\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mECDF\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mECDFDiscrete\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmonotone_fn_inverter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mStepFunction\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     )\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0medgeworth\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mExpandedNormal\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\statsmodels\\distributions\\empirical_distribution.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \"\"\"\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minterpolate\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0minterp1d\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\learn-env\\lib\\site-packages\\scipy\\interpolate\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    163\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mshould\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mused\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnew\u001b[0m \u001b[0mcode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    164\u001b[0m \"\"\"\n\u001b[1;32m--> 165\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0minterpolate\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    166\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mfitpack\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    167\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\learn-env\\lib\\site-packages\\scipy\\interpolate\\interpolate.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m                    ravel, poly1d, asarray, intp)\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mspecial\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mspec\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mspecial\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcomb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_util\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mprod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\learn-env\\lib\\site-packages\\scipy\\special\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    634\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_ufuncs\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    635\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 636\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_basic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    637\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_basic\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    638\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\learn-env\\lib\\site-packages\\scipy\\special\\_basic.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m                       poch, binom, hyp0f1)\n\u001b[0;32m     15\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mspecfun\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0morthogonal\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_comb\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_comb_int\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\learn-env\\lib\\site-packages\\scipy\\special\\orthogonal.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[1;31m# SciPy imports.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 79\u001b[1;33m from numpy import (exp, inf, pi, sqrt, floor, sin, cos, around, int,\n\u001b[0m\u001b[0;32m     80\u001b[0m                    hstack, arccos, arange)\n\u001b[0;32m     81\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlinalg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'int' from 'numpy' (C:\\Users\\micha\\anaconda3\\envs\\learn-env\\lib\\site-packages\\numpy\\__init__.py)"
     ]
    }
   ],
   "source": [
    "# import necessary packages\n",
    "import warnings\n",
    "from statsmodels.tools.sm_exceptions import ConvergenceWarning\n",
    "warnings.simplefilter('ignore', ConvergenceWarning)\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "from matplotlib.pylab import rcParams\n",
    "from statsmodels.tsa.stattools import adfuller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read dataset into pandas df\n",
    "zillow_df = pd.read_csv('zillow_data.csv')\n",
    "zillow_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter dataset so it only includes zipcodes in Nevada \n",
    "nv_df = zillow_df[zillow_df['State']=='NV']\n",
    "nv_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that I have the filtered dataset, <b>nv_df</b>, I will explore the data further by looking at the <b>county</b> that has the most zipcodes. Using <b>.value_counts()</b>, I see that <b>Clark County</b> has the most zipcodes by far. I will therefore create another dataframe, <b>clark_df</b>, that focuses on the <b>61 zipcodes</b> in Clark County. I will also look at the distribution of Clark County zipcodes by city, once again employing the <b>.value_counts()</b> method. From <b>.value_counts()</b>, we see that <b>Las Vegas</b> is the city where most of the zipcodes are located in, accounting for <b>38 zipcodes</b> out of the 61 Clark County zipcodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looking at distribution of zipcodes amongst counties in Nevada\n",
    "nv_df['CountyName'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# creating dataframe for the zipcodes in Clark County\n",
    "clark_df = nv_df[nv_df['CountyName']=='Clark']\n",
    "clark_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looking at distribution of zipcodes amongst cities in Clark County\n",
    "clark_df['City'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One important thing that must be done to the <b>clark_df</b> prior to plotting it is to melt it. The dataframe right now is in wide form and must be converted to long form. The function <b>melt_data()</b> can be used to achieve this melting of <b>clark_df</b>. The returned dataframe will have <b>time</b> as the index and an aggregate value in the <b>value</b> column. From the <b>melt_data()</b> function, we see that the returned aggregate value is the <b>mean</b>. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# melt_data helper function provided in starter notebook\n",
    "# converts dataframe from wide-form to long-form\n",
    "\n",
    "def melt_data(df):\n",
    "    \"\"\"\n",
    "    Takes the zillow_data dataset in wide form or a subset of the zillow_dataset.  \n",
    "    Returns a long-form datetime dataframe \n",
    "    with the datetime column names as the index and the values as the 'values' column.\n",
    "    \n",
    "    If more than one row is passes in the wide-form dataset, the values column\n",
    "    will be the mean of the values from the datetime columns in all of the rows.\n",
    "    \"\"\"\n",
    "    \n",
    "    melted = pd.melt(df, id_vars=['RegionName', 'RegionID', 'SizeRank', 'City', 'State', 'Metro', 'CountyName'], var_name='time')\n",
    "    melted['time'] = pd.to_datetime(melted['time'], infer_datetime_format=True)\n",
    "    melted = melted.dropna(subset=['value'])\n",
    "    return melted.groupby('time').aggregate({'value':'mean'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "melted_clark_df = melt_data(clark_df)\n",
    "melted_clark_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Series Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the dataframe has been melted, we can plot <b>melted_clark_df</b>. From the graph, we see that for all zipcodes in Clark County, there was an increase in price between 1996 and 2006. However, around the 2007 and 2008 mark, the prices began to dip. It was not until around 2013 that the housing prices started to really go up again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting melted_clark_df\n",
    "melted_clark_df.plot(figsize=(18,10), subplots=False, legend=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot above aggregates data from all the zipcodes into a single graph. I want to now look at the general trends for each of the 61 zipcodes in <b>clark_df</b> individually. From the graph below, we see that the pattern for each of the zipcodes is pretty similar. The 2007-2008 mark is when housing prices started to drop significantly. Then, around 2013, the housing prices started to really climb again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code adapted from Sanjit Varma \n",
    "# Source: https://github.com/sanjitva/Zillow-TimeSeries-Modeling/blob/main/final_notebook.ipynb\n",
    "\n",
    "# plotting time series for all zipcodes in clark_df\n",
    "\n",
    "# extracting zipcodes from clark_df\n",
    "clark_zips = [a for a in clark_df['RegionName']]\n",
    "\n",
    "# initialize empty dict\n",
    "zip_dict = {}\n",
    "\n",
    "# iterate over every zipcode in clark_zips\n",
    "# use melt_data helper function to put data in long form\n",
    "for zipcode in clark_zips:\n",
    "    zip_dict[zipcode] = melt_data(clark_df[clark_df['RegionName']==zipcode])\n",
    "\n",
    "# plot time series data\n",
    "fig, ax = plt.subplots(figsize=(20,12))\n",
    "\n",
    "for zipcode in zip_dict:\n",
    "    ax.plot(zip_dict[zipcode],)  \n",
    "\n",
    "ax.set_title('Zipcode Price Changes (1996-2018)', fontsize=20)\n",
    "ax.set_xlabel('Year', fontsize=15)\n",
    "ax.set_ylabel('Price (USD)', fontsize=15);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prior to creating a time series model, I want to look at ROI since 2013. Given that the dataset goes on until 2018, this would be a <b>5-year ROI</b>. I will add a column to the <b>clark_df</b> that stores this calculated <b>5-year ROI</b>. Once I have the <b>5-year ROI</b> column made in the original <b>clark_df</b> dataframe, I will then make a new dataframe called <b>clark_df_filtered</b>, which only houses zipcodes that have an <b>ROI greater than 1.0 (100%)</b>. From there, I will plot the zipcodes in <b>clark_df_filtered</b> with their corresponding ROI values converted to a percentage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating 5_yr_ROI col in clark_df\n",
    "clark_df['5_yr_ROI'] = (clark_df['2018-04'] - clark_df['2013-04'])/(clark_df['2013-04'])\n",
    "clark_df['5_yr_ROI']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating filtered clark_df that only includes zipcodes that have 5_yr_ROI greater than 1.0 (100%)\n",
    "clark_df_filtered = clark_df[clark_df['5_yr_ROI']>=1.0]\n",
    "clark_df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code adapted from Sanjit Varma \n",
    "# Source: https://github.com/sanjitva/Zillow-TimeSeries-Modeling/blob/main/final_notebook.ipynb\n",
    "\n",
    "# plotting ROI for each zipcode\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(60,50))\n",
    "\n",
    "x_labels = [str(a) for a in clark_df_filtered['RegionName']]\n",
    "x = list(range(1,11))\n",
    "y = [a for a in clark_df_filtered['5_yr_ROI']]\n",
    "\n",
    "ax.bar(x,y)\n",
    "\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(x_labels)\n",
    "ax.set_yticks([a/10 for a in list(range(0,15,1))])\n",
    "ax.set_yticklabels([str(a*10)+'%' for a in list(range(0,15,1))])\n",
    "ax.set_ylabel('Growth (%)', fontsize='30')\n",
    "ax.set_xlabel('Zipcodes', fontsize='30')\n",
    "ax.set_title('Avg % Return on Investment',fontsize='40');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The zipcode with the greatest ROI is <b>89104</b>, which has an <b>ROI of about 130%</b>. We can utilize the data for this zipcode to create our initial time series model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation: 89104 Zipcode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prior to modeling, we need to prepare our data for the <b>89104</b> zipcode. I will begin by making a new dataframe that houses the 89104 zipcode data. From there, I will employ the <b>melt_data_2()</b> function to convert the dataframe from wide form to long form. The <b>melt_data_2()</b> function is set up exactly like the <b>melt_data()</b> helper function. The only difference is that <b>melt_data_2()</b> factors in the new <b>5_yr_ROI</b> column that was added earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same as melt_data helper function \n",
    "# just added '5_yr_ROI' under id_vars\n",
    "\n",
    "def melt_data_2(df):\n",
    "    \"\"\"\n",
    "    Takes the zillow_data dataset in wide form or a subset of the zillow_dataset.  \n",
    "    Returns a long-form datetime dataframe \n",
    "    with the datetime column names as the index and the values as the 'values' column.\n",
    "    \n",
    "    If more than one row is passes in the wide-form dataset, the values column\n",
    "    will be the mean of the values from the datetime columns in all of the rows.\n",
    "    \"\"\"\n",
    "    \n",
    "    melted = pd.melt(df, id_vars=['RegionName', 'RegionID', 'SizeRank', 'City', 'State', 'Metro', 'CountyName', '5_yr_ROI'], var_name='time')\n",
    "    melted['time'] = pd.to_datetime(melted['time'], infer_datetime_format=True)\n",
    "    melted = melted.dropna(subset=['value'])\n",
    "    return melted.groupby('time').aggregate({'value':'mean'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make new df housing only 89104 zipcode data, then use melt_data to convert to long form\n",
    "zipcode_89104_data = clark_df_filtered[clark_df_filtered['RegionName']==89104]\n",
    "tseries_89104 = melt_data_2(zipcode_89104_data)\n",
    "tseries_89104"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the melted <b>tseries_89104</b> data, our next step prior to modeling is checking for <b>seasonality</b> and <b>trends</b>. I plan to build an <b>ARIMA model</b>. Therefore, the assumptions of the <b>ARIMA model</b> are that the time series data is <b>non-seasonal</b> and <b>detrended</b>. If these assumptions are not met, we need to do more data preparation prior to building our model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seasonality and Trends\n",
    "\n",
    "To check for trends and seasonality, we need to use methods that allow us to understand a time series' <b>stationarity</b>. In order for the assumptions of <b>non-seasonality</b> and <b>detrending</b> to be met, the data must be deemed <b>non-stationary</b>. Two ways to determine stationarity are <b>rolling statistics</b> and the <b>Dickey-Fuller Test</b>. I plan to use both of these methods on <b>tseries_89104</b>. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting tseries_89104 to visualize it\n",
    "tseries_89104.plot(figsize=(12,6), linewidth=2, fontsize=10);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check rolling statistics\n",
    "\n",
    "# determine rolling statistics\n",
    "roll_mean = tseries_89104.rolling(window=12, center=False).mean()\n",
    "roll_std = tseries_89104.rolling(window=12, center=False).std()\n",
    "\n",
    "# plot rolling statistics\n",
    "fig = plt.figure(figsize=(12,6))\n",
    "plt.plot(tseries_89104, color='blue',label='Original')\n",
    "plt.plot(roll_mean, color='red', label='Rolling Mean')\n",
    "plt.plot(roll_std, color='black', label = 'Rolling Std')\n",
    "plt.legend(loc='best')\n",
    "plt.title('Rolling Mean & Standard Deviation for 89104 Zipcode')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform Dickey-Fuller test\n",
    "print ('Results of Dickey-Fuller Test: \\n')\n",
    "dftest = adfuller(tseries_89104['value'])\n",
    "\n",
    "# extract and display test results\n",
    "dfoutput = pd.Series(dftest[0:4], index=['Test Statistic', 'p-value', '#Lags Used', 'Number of Observations Used'])\n",
    "for key, value in dftest[4].items():\n",
    "    dfoutput['Critical Value (%s)'%key] = value\n",
    "print(dfoutput)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From our <b>rolling statistics</b> analysis, we see that the <b>rolling mean</b> is <b>not constant over time</b>, thus indicating <b>non-stationarity</b>. This <b>non-stationary</b> behavior is further supported by the <b>Dickey-Fuller Test</b> results, whose test statistic has an insignificant <b>p-value greater than 0.05</b>. This means that the null hypothesis, which states that the <b>time series is non-stationary</b> cannot be rejected. Therefore, more processing of the data must be done before getting into modeling. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing Seasonality and Trends"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are numerous methods that can be employed to remove seasonality and trends in a time series. The methods I want to use for <b>tseries_89104</b> are <b>subtracting rolling mean</b>, <b>subtracting weighted rolling mean</b>, <b>log transform</b> and <b>square root transform</b>. I will create visualizations for the time series when all four of these strategies are applied. I will also perform the <b>Dickey Fuller Test</b> for these methods to confirm non-seasonality. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subtract rolling mean\n",
    "tseries_89104_minus_roll_mean = tseries_89104 - roll_mean\n",
    "tseries_89104_minus_roll_mean.dropna(inplace=True)\n",
    "\n",
    "fig = plt.figure(figsize=(11,7))\n",
    "plt.plot(tseries_89104_minus_roll_mean, color='blue', label='value - rolling mean')\n",
    "plt.legend(loc='best')\n",
    "plt.title('Zipcode 89104 Data (Subtract Rolling Mean)')\n",
    "plt.show(block=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform Dickey-Fuller test for rolling mean results\n",
    "print ('Results of Dickey-Fuller Test: \\n')\n",
    "dftest = adfuller(tseries_89104_minus_roll_mean['value'])\n",
    "\n",
    "# extract and display test results\n",
    "dfoutput = pd.Series(dftest[0:4], index=['Test Statistic', 'p-value', '#Lags Used', 'Number of Observations Used'])\n",
    "for key, value in dftest[4].items():\n",
    "    dfoutput['Critical Value (%s)'%key] = value\n",
    "print(dfoutput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weighted rolling mean\n",
    "\n",
    "# use Pandas ewm() to calculate Exponential Weighted Moving Average\n",
    "exp_roll_mean = tseries_89104.ewm(halflife=2).mean()\n",
    "\n",
    "# subtract the moving average from the original data\n",
    "tseries_89104_minus_exp_roll_mean = tseries_89104 - exp_roll_mean\n",
    "\n",
    "# plot\n",
    "fig = plt.figure(figsize=(11,7))\n",
    "plt.plot(tseries_89104_minus_exp_roll_mean, color='blue', label='value - rolling mean')\n",
    "plt.legend(loc='best')\n",
    "plt.title('Zipcode 89104 Data (Subtract Weighted Rolling Mean)')\n",
    "plt.show(block=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform Dickey-Fuller test for weighted rolling mean results\n",
    "print ('Results of Dickey-Fuller Test: \\n')\n",
    "dftest = adfuller(tseries_89104_minus_exp_roll_mean['value'])\n",
    "\n",
    "# extract and display test results\n",
    "dfoutput = pd.Series(dftest[0:4], index=['Test Statistic', 'p-value', '#Lags Used', 'Number of Observations Used'])\n",
    "for key, value in dftest[4].items():\n",
    "    dfoutput['Critical Value (%s)'%key] = value\n",
    "print(dfoutput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot a log transform\n",
    "tseries_89104_log = np.log(tseries_89104)\n",
    "fig = plt.figure(figsize=(12,6))\n",
    "plt.plot(tseries_89104_log, color='blue');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform Dickey-Fuller test for log transform results\n",
    "print ('Results of Dickey-Fuller Test: \\n')\n",
    "dftest = adfuller(tseries_89104_log['value'])\n",
    "\n",
    "# extract and display test results\n",
    "dfoutput = pd.Series(dftest[0:4], index=['Test Statistic', 'p-value', '#Lags Used', 'Number of Observations Used'])\n",
    "for key, value in dftest[4].items():\n",
    "    dfoutput['Critical Value (%s)'%key] = value\n",
    "print(dfoutput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot a square root transform\n",
    "tseries_89104_sqrt = np.sqrt(tseries_89104)\n",
    "fig = plt.figure(figsize=(12,6))\n",
    "plt.plot(tseries_89104_sqrt, color='blue');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform Dickey-Fuller test for square root transform results\n",
    "print ('Results of Dickey-Fuller Test: \\n')\n",
    "dftest = adfuller(tseries_89104_sqrt['value'])\n",
    "\n",
    "# extract and display test results\n",
    "dfoutput = pd.Series(dftest[0:4], index=['Test Statistic', 'p-value', '#Lags Used', 'Number of Observations Used'])\n",
    "for key, value in dftest[4].items():\n",
    "    dfoutput['Critical Value (%s)'%key] = value\n",
    "print(dfoutput)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After applying all four methods, we see that the best results are yielded by the <b>log transformed</b> and <b>square root transformed</b> time series. The <b>p-values</b> for the <b>Dickey-Fuller Tests</b> for these methods are <b>about 0.01</b>, indicating significance at a threshold of 0.05 and <b>non-seasonality</b>. When <b>subtracting the rolling mean</b> and <b>the weighted rolling mean</b>, the <b>Dickey-Fuller Tests</b> have <b>p-values</b> that are <b>insignificant</b>, being about <b>0.24</b> and <b>0.12</b>, respectively. I will proceed with <b>tseries_89104_log</b> and <b>tseries_89104_sqrt</b> use them for my <b>ARIMA modeling</b>. Based on the model results, I will choose the one that I want to utilize for forecasting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ARIMA Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the data is <b>non-seasonal</b> and <b>detrended</b>, we can begin the iterative modeling process. Remember, we're using <b>tseries_89104_log</b> and <b>tseries_89104_sqrt</b>. Let's go ahead and split <b>tseries_89104_log</b> and <b>tseries_89104_sqrt</b> into training and test sets. The training sets will house data from April 2011 to April 2016, while the test sets will be information from May 2016 to April 2018, which is where the dataset ends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split tseries_89104_log and tseries_89104_sqrt into training and test sets\n",
    "tseries_89104_log_train = tseries_89104_log['2011-04':'2016-04'] \n",
    "tseries_89104_log_test = tseries_89104_log['2016-05':] \n",
    "tseries_89104_sqrt_train = tseries_89104_sqrt['2011-04':'2016-04']\n",
    "tseries_89104_sqrt_test = tseries_89104_sqrt['2016-05':]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now turn our attention to creating our models, which we will fit on <b>tseries_89104_log_train</b> and <b>tseries_89104_sqrt_train</b>. An <b>ARIMA model</b> can be fit on time series data to predict future points. The parameters for ARIMA are specified as <b>(p,d,q)</b>, where p is the autoregressive part of the model, d is the integrated component, and q is the moving average portion of the model. (p,d,q) represents the model's <b>order</b>. In order to find the <b>optimal parameters</b> for (p,d,q), a <b>grid search method</b> can be employed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up grid search\n",
    "\n",
    "# Define the p, d and q parameters to take any value between 0 and 2\n",
    "p = d = q = range(0, 2)\n",
    "\n",
    "# Generate all different combinations of p, q and q triplets\n",
    "pdq = list(itertools.product(p, d, q))\n",
    "\n",
    "# Generate all different combinations of seasonal p, q and q triplets\n",
    "pdqs = [(x[0], x[1], x[2], 12) for x in list(itertools.product(p, d, q))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ARIMA Modeling: tseries_89104_log_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a grid with pdq and seasonal pdq parameters and get the best AIC value\n",
    "# For tseries_89104_log_train\n",
    "ans_log = []\n",
    "for comb in pdq:\n",
    "    for combs in pdqs:\n",
    "        try:\n",
    "            mod = sm.tsa.statespace.SARIMAX(tseries_89104_log_train,\n",
    "                                            order=comb,\n",
    "                                            seasonal_order=combs,\n",
    "                                            enforce_stationarity=False,\n",
    "                                            enforce_invertibility=False)\n",
    "\n",
    "            output = mod.fit()\n",
    "            ans_log.append([comb, combs, output.aic])\n",
    "            print('ARIMA {} x {}: AIC Calculated={}'.format(comb, combs, output.aic))\n",
    "        except:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the parameters with minimal AIC value for tseries_89104_log_train\n",
    "ans_df_log = pd.DataFrame(ans_log, columns=['pdq', 'pdqs', 'aic'])\n",
    "ans_df_log.loc[ans_df_log['aic'].idxmin()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the numbers for <b>pdq</b> and <b>pdqs</b> selected by our grid search, we can now plug in these optimal value combinations for our <b>order</b> and <b>seasonal_order</b> parameters. Let's run the model again and look at our results. We will also look at different visualizations using the <b>.plot_diagnostics()</b> method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plug the optimal parameter values into a new SARIMAX model\n",
    "# For tseries_89104_log_train\n",
    "ARIMA_MODEL = sm.tsa.statespace.SARIMAX(tseries_89104_log_train, \n",
    "                                        order=(1, 1, 1), \n",
    "                                        seasonal_order=(0, 0, 0, 12), \n",
    "                                        enforce_stationarity=False, \n",
    "                                        enforce_invertibility=False)\n",
    "\n",
    "# Fit the model and print results\n",
    "output_log = ARIMA_MODEL.fit()\n",
    "\n",
    "print(output_log.summary().tables[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call plot_diagnostics() on tseries_89104_log_train\n",
    "output_log.plot_diagnostics(figsize=(15, 18))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the ARIMA model summary, we see that all coefficients are significant. From the visualizations generated by <b>.plot_diagnostics()</b>, we notice that the <b>KDE follows a relatively normal distribution</b> and matches up closely with the N(0,1) standard normal distribution curve. Though some of the points fall off the red guidance line, the <b>qq-plot</b> has dots that show a relatively linear trend. This indicates that the residuals are normally distributed. The top left plot shows no obvious seasonality; this is further supported by the <b>correlogram</b> on the bottom right, which tells us that the residuals in the time series have low correlations with the lagged versions of itself. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ARIMA Modeling: tseries_89104_sqrt_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to what we did with <b>tseries_89104_log_train</b>, we will find the optimal parameters for pdq and seasonal pdq for <b>tseries_89104_sqrt_train</b> and fit an optimized ARIMA model on it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a grid with pdq and seasonal pdq parameters and get the best AIC value\n",
    "# For tseries_89104_sqrt_train\n",
    "ans_sqrt = []\n",
    "for comb in pdq:\n",
    "    for combs in pdqs:\n",
    "        try:\n",
    "            mod = sm.tsa.statespace.SARIMAX(tseries_89104_sqrt_train,\n",
    "                                            order=comb,\n",
    "                                            seasonal_order=combs,\n",
    "                                            enforce_stationarity=False,\n",
    "                                            enforce_invertibility=False)\n",
    "\n",
    "            output = mod.fit()\n",
    "            ans_sqrt.append([comb, combs, output.aic])\n",
    "            print('ARIMA {} x {}: AIC Calculated={}'.format(comb, combs, output.aic))\n",
    "        except:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the parameters with minimal AIC value for tseries_89104_sqrt_train\n",
    "ans_df_sqrt = pd.DataFrame(ans_sqrt, columns=['pdq', 'pdqs', 'aic'])\n",
    "ans_df_sqrt.loc[ans_df_sqrt['aic'].idxmin()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run the ARIMA model again with the grid search-selected parameters and look at our results. We will print out the model table summary and employ <b>plot_diagnostics()</b>, similar to what we did with the log times series. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plug the optimal parameter values into a new SARIMAX model\n",
    "# For tseries_89104_sqrt_train\n",
    "ARIMA_MODEL = sm.tsa.statespace.SARIMAX(tseries_89104_sqrt_train, \n",
    "                                        order=(1, 1, 0), \n",
    "                                        seasonal_order=(1, 1, 0, 12), \n",
    "                                        enforce_stationarity=False, \n",
    "                                        enforce_invertibility=False)\n",
    "\n",
    "# Fit the model and print results\n",
    "output_sqrt = ARIMA_MODEL.fit()\n",
    "\n",
    "print(output_sqrt.summary().tables[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call plot_diagnostics() on tseries_89104_sqrt_train\n",
    "output_sqrt.plot_diagnostics(figsize=(15, 18))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the ARIMA model summary, we see that all of the coefficients are significant. From the visualizations generated by <b>.plot_diagnostics()</b>, we notice that the <b>KDE follows a relatively normal distribution</b> and matches up well with the N(0,1) curve; however, there is a slight concave portion at the top. While some of the points fall off the red guidance line in the <b>qq-plot</b>, the blue dots still show a relatively linear trend. This indicates that the residuals are normally distributed. The top left plot shows no obvious seasonality; this is also supported by the <b>correlogram</b>, which tells us that the residuals in the time series have low correlations with the lagged versions of itself. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ARIMA Modeling: tseries_89104"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Earlier, we said that the original <b>tseries_89104</b> data exhibited some seasonality based on the Dickey-Fuller test results, which yielded an insignificant p-value of <b>about 0.09</b>. However, for comparison purposes, let's also create an ARIMA model for the raw <b>tseries_89104</b> time series. Similar to the modeling done for the previous two time series, <b>tseries_89104_log_train</b> and <b>tseries_89104_sqrt_train</b>, we will use a grid search to determine the optimal parameters. It should also be noted that we will fit the model on <b>tseries_89104_train</b>, which houses data between April 2011 and April 2016. The test set will be data from May 2016 to April 2018, just like how we set it up for the log transformed and square root transformed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split tseries_89104 into training and test set\n",
    "tseries_89104_train = tseries_89104['2011-04':'2016-04']\n",
    "tseries_89104_test = tseries_89104['2016-05':]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a grid with pdq and seasonal pdq parameters and get the best AIC value\n",
    "# For tseries_89104_train\n",
    "ans = []\n",
    "for comb in pdq:\n",
    "    for combs in pdqs:\n",
    "        try:\n",
    "            mod = sm.tsa.statespace.SARIMAX(tseries_89104_train,\n",
    "                                            order=comb,\n",
    "                                            seasonal_order=combs,\n",
    "                                            enforce_stationarity=False,\n",
    "                                            enforce_invertibility=False)\n",
    "\n",
    "            output = mod.fit()\n",
    "            ans.append([comb, combs, output.aic])\n",
    "            print('ARIMA {} x {}: AIC Calculated={}'.format(comb, combs, output.aic))\n",
    "        except:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the parameters with minimal AIC value for tseries_89104_train\n",
    "ans_df = pd.DataFrame(ans, columns=['pdq', 'pdqs', 'aic'])\n",
    "ans_df.loc[ans_df['aic'].idxmin()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plug the optimal parameter values into a new SARIMAX model \n",
    "# For tseries_89104_train\n",
    "ARIMA_MODEL = sm.tsa.statespace.SARIMAX(tseries_89104_train, \n",
    "                                        order=(1, 1, 0), \n",
    "                                        seasonal_order=(1, 1, 0, 12), \n",
    "                                        enforce_stationarity=False, \n",
    "                                        enforce_invertibility=False)\n",
    "\n",
    "# Fit the model and print results\n",
    "output = ARIMA_MODEL.fit()\n",
    "\n",
    "print(output.summary().tables[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Call plot_diagnostics() on tseries_89104_train\n",
    "output.plot_diagnostics(figsize=(15, 18))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the ARIMA model summary, we see that all of the coefficients are significant. From the visualizations generated by <b>.plot_diagnostics()</b>, we notice that the <b>KDE follows a relatively normal distribution</b> and matches up closely with the N(0,1) curve. While some of the points fall off the red guidance line in the <b>qq-plot</b>, the blue dots still show a relatively linear trend, indicating that the residuals are normally distributed. The top left plot shows no obvious seasonality; this is also supported by the <b>correlogram</b>, which tells us that the residuals in the time series have low correlations with the lagged versions of itself. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choosing Model\n",
    "\n",
    "Between the three ARIMA models for <b>tseries_89104_log_train</b>, <b>tseries_89104_sqrt_train</b>, and <b>tseries_89104_train</b>, I will proceed with the model for <b>tseries_89104_train</b>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validating Model \n",
    "\n",
    "Now that we've deemed the fit of the model to be satisfactory, we can validate the chosen model using predicted values to compare to real values in the time series. This will give us insight on the accuracy of our forecasts. I will employ the <b>get_prediction()</b> and <b>conf_int()</b> methods to achieve this model validation. In terms of forecasting, I will use <b>one-step ahead forecasting</b>. The metric I will utilize to determine the accuracy of the forecasts will be <b>MSE</b>. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One-Step Ahead Forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions starting from 2016-05 and calculate confidence intervals\n",
    "pred = output.get_prediction(start=pd.to_datetime('2016-05'), end=pd.to_datetime('2018-04'), dynamic=False)\n",
    "pred_conf = pred.conf_int()\n",
    "pred_conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot real vs predicted values along with confidence interval\n",
    "\n",
    "rcParams['figure.figsize'] = 15, 6\n",
    "\n",
    "# Plot observed values\n",
    "ax = tseries_89104['2014':].plot(label='observed')\n",
    "\n",
    "# Plot predicted values\n",
    "pred.predicted_mean.plot(ax=ax, label='One-Step Ahead Forecast', alpha=0.9)\n",
    "\n",
    "# Plot the range for confidence intervals\n",
    "ax.fill_between(pred_conf.index,\n",
    "                pred_conf.iloc[:, 0],\n",
    "                pred_conf.iloc[:, 1], color='g', alpha=0.5, label='Confidence Interval')\n",
    "\n",
    "# Set axes labels\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Price')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on model validation using the test set, we see that the confidence intervals get bigger the farther we go along the time series. This is an indication that the model has room for improvement. However, for this particular project, we will proceed with it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "#mae = mean_absolute_error(actual_values, predicted_values)\n",
    "#print(mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forecasting Future (Zipcode 89104)\n",
    "\n",
    "Now that we have tested our model, we can fit it onto the entire <b>tseries_89104</b> dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit entire tseries_89104\n",
    "# use same order and seasonal order parameters\n",
    "ARIMA_MODEL = sm.tsa.statespace.SARIMAX(tseries_89104, \n",
    "                                        order=(1, 1, 0), \n",
    "                                        seasonal_order=(1, 1, 0, 12), \n",
    "                                        enforce_stationarity=False, \n",
    "                                        enforce_invertibility=False)\n",
    "\n",
    "# fit model and print results\n",
    "output_tseries_89104 = ARIMA_MODEL.fit()\n",
    "\n",
    "print(output_tseries_89104.summary().tables[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting a forecast for the next 5 years (60 months) after April 2018 (last record in time series data)\n",
    "forecast = output_tseries_89104.get_forecast(60)\n",
    "future_prediction = forecast.conf_int()\n",
    "future_prediction['value'] = forecast.predicted_mean\n",
    "future_prediction.columns = ['lower','upper','prediction'] \n",
    "future_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plotting our forecast\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "tseries_89104.plot(ax=ax, label='Real Values')\n",
    "\n",
    "\n",
    "future_prediction['prediction'].plot(ax=ax, label='predicted value', ls='--')\n",
    "\n",
    "ax.fill_between(x=future_prediction.index, \n",
    "                y1=future_prediction['lower'], \n",
    "                y2=future_prediction['upper'], \n",
    "                color='lightpink',\n",
    "                label='Confidence Interval')\n",
    "ax.legend() \n",
    "plt.ylabel(\"Average Price\")\n",
    "plt.title(\"Forecasted House Prices (Zipcode 89104)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forecasting Future (All Zipcodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like we used our model to forecast 89104 zipcode prices, we can now apply this model that we created to all the zipcodes in <b>clark_df_filtered</b>. Recall that this dataframe includes zipcodes in Clark County that had a 5-year ROI of 1.0 or greater. Using a function, <b>forecast_model()</b>, which has the <b>order</b> and <b>seasonal order</b> parameters set to the ones used in the final ARIMA model, we can apply the ARIMA model we made earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting zipcodes from clark_df\n",
    "clark_zips = [a for a in clark_df_filtered['RegionName']]\n",
    "\n",
    "# initialize empty dict\n",
    "zip_dict = {}\n",
    "\n",
    "# iterate over every zipcode in clark_zips\n",
    "# use melt_data_2 function to put data in long form\n",
    "for zipcode in clark_zips:\n",
    "    zip_dict[zipcode] = melt_data_2(clark_df_filtered[clark_df_filtered['RegionName']==zipcode])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code adapted from Fernando Aguilar\n",
    "# Source: https://medium.com/@feraguilari/time-series-analysis-modfinalproyect-b9fb23c28309\n",
    "\n",
    "def forecast_model(df, pdq=(1, 1, 0), pdqs=(1, 1, 0, 12), display=True, zc='input zipcode'):\n",
    "    model = sm.tsa.statespace.SARIMAX(df, order=pdq, seasonal_order=pdqs)\n",
    "    model_fit = model.fit()\n",
    "    output = model_fit.get_prediction(start='2018-04', end='2023-04', dynamic=True)\n",
    "    forecast_ci = output.conf_int()\n",
    "    if display:\n",
    "        fig, ax = plt.subplots(figsize=(13,6))\n",
    "        output.predicted_mean.plot(label='Forecast')\n",
    "        ax.fill_between(forecast_ci.index,forecast_ci.iloc[:, 0],forecast_ci.iloc[:, 1],\n",
    "                        color='k', alpha=.25,label='Conf Interval')\n",
    "        plt.title('Forecast of Monthly Returns')\n",
    "        plt.xlabel('Time')\n",
    "        plt.legend(loc='best')\n",
    "        plt.show()\n",
    "    year_1= (1+output.predicted_mean[:12]).prod()-1\n",
    "    year_3=(1+output.predicted_mean[:36]).prod()-1\n",
    "    year_5= (1+output.predicted_mean[:60]).prod()-1\n",
    "    print(f'Total expected return in 1 year: {round(year_1*100,2)}%')\n",
    "    print(f'Total expected return in 3 years: {round(year_3*100,2)}%')\n",
    "    print(f'Total expected return in 5 year: {round(year_5*100,2)}%')\n",
    "    tot_ret = [zc, year_1, year_3, year_5]\n",
    "    return tot_ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_model(tseries_89104)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommendations\n",
    "\n",
    "Find Top 5 zipcodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Limitations and Improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final model used for forecasting had many issues (wide confidence intervals). Can only be used to project small amounts of time (still useful in this sense). \n",
    "\n",
    "Model for determining best zipcode can be improved \n",
    "\n",
    "Recommendations still solid because the zipcodes are in the Top 10 of ROI from 2013-2018 (assuming continuing trend)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
